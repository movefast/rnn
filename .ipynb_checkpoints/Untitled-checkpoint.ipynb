{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import agent\n",
    "# from q_learning_agent import QLearningAgent\n",
    "from gridworld_with_door import MazeEnvironment\n",
    "from tqdm import tqdm\n",
    "# from lin_agent_tiles import LinearAgent\n",
    "from nn_agent import LinearAgent as NNAgent\n",
    "# from rnn_agent_n_step import RNNAgent\n",
    "from rnn_agent import RNNAgent as RNNAgent\n",
    "from sarsa_agent import SarsaAgent\n",
    "from q_learning_agent import QLearningAgent\n",
    "from IPython.display import HTML, Image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "# enable if ffmpeg codec is installed \n",
    "# plt.rcParams['animation.ffmpeg_path'] = '/anaconda3/envs/pytorch/bin/ffmpeg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 15})\n",
    "plt.rcParams.update({'figure.figsize': [10,5]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Limitation RNN GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buffer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### state dist investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, agent, state_visits=None, keep_history=False):\n",
    "    is_terminal = False\n",
    "    sum_of_rewards = 0\n",
    "    step_count = 0\n",
    "    \n",
    "    obs = env.env_start(keep_history=keep_history)\n",
    "    action = agent.agent_start(obs)\n",
    "    \n",
    "    if state_visits is not None:\n",
    "        state_visits[obs[0]] += 1\n",
    "\n",
    "    while not is_terminal:\n",
    "        reward, obs, is_terminal = env.env_step(action)\n",
    "        print(agent.steps,end='\\r')\n",
    "        sum_of_rewards -= 1\n",
    "        step_count += 1\n",
    "        state = obs\n",
    "        if step_count == 500:\n",
    "            agent.agent_end(reward, state, append_buffer=False)\n",
    "            break\n",
    "        elif is_terminal:\n",
    "            agent.agent_end(reward, state, append_buffer=True)\n",
    "        else:\n",
    "            action = agent.agent_step(reward, state)\n",
    "\n",
    "        if state_visits is not None:\n",
    "            state_visits[state[0]] += 1\n",
    "    \n",
    "    if keep_history:\n",
    "        history = env.history\n",
    "        env.env_cleanup()\n",
    "        return sum_of_rewards, history\n",
    "    else:\n",
    "        return sum_of_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate(history, name='history.gif'):\n",
    "    frames = len(history)\n",
    "    print(f\"Rendering {frames} frames...\")\n",
    "    fig = plt.figure(figsize=(6, 2))\n",
    "    fig_grid = fig.add_subplot(121)\n",
    "\n",
    "    def render_frame(i):\n",
    "        grid = history[i]\n",
    "        fig_grid.matshow(grid, vmin=-1, vmax=1, cmap='jet')\n",
    "    anim = animation.FuncAnimation(fig, render_frame, frames=frames, interval=100);\n",
    "    plt.close(anim._fig)\n",
    "    # Option a) if ffmpeg codec is installed, display animation with ffmpeg\n",
    "    # display(HTML(anim.to_html5_video()))\n",
    "    # Option b) save as gif and display\n",
    "    anim.save(name, dpi=80, writer=animation.PillowWriter(fps=20));\n",
    "    with open(name,'rb') as file:\n",
    "        display(Image(file.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload(obj):\n",
    "   import inspect\n",
    "   import imp\n",
    "   cur_mod = inspect.getmodule(obj)\n",
    "   imp.reload(cur_mod)\n",
    "   mod_name = cur_mod.__name__\n",
    "   obj_name = obj.__name__\n",
    "   #from mod_name import obj_name as obj\n",
    "   return getattr(__import__(mod_name, fromlist=[obj_name]), obj_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNNAgent = reload(RNNAgent)\n",
    "RNNAgentM = reload(RNNAgentM)\n",
    "NNAgent = reload(NNAgent)\n",
    "\n",
    "agents = {\n",
    "    \"NN\": NNAgent,\n",
    "    \"RNN\": RNNAgentM,\n",
    "    'TBPTT_Multi': RNNAgentM,\n",
    "    \"GRU\": RNNAgentGRU,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = {\n",
    "    'Grid-World': MazeEnvironment,\n",
    "}\n",
    "agent_infos = {\n",
    "    \"Q-learning\": {\"step_size\": .5},\n",
    "    \"Sarsa\": {\"step_size\": 1e-2, 'num_tilings': 4, 'num_tiles': 4, 'iht_size': 300},\n",
    "    \"Linear\": {\"step_size\": 1e-3},\n",
    "    \"NN\": {\"step_size\": 1e-3},\n",
    "    \"RNN\": {\"step_size\": 1e-3},\n",
    "    \"GRU\": {\"step_size\": 1e-3}\n",
    "}\n",
    "env_info = {\n",
    "    \"maze_dim\": [7, 7], \n",
    "    \"start_state\": [6, 0], \n",
    "    \"end_state\": [6, 6],\n",
    "    \"obstacles\": [[3, 3], [3, 5], [3, 6], [4, 3], [5, 3], [6, 3]],\n",
    "    \"doors\": {tuple([6,0]):[3,4]},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reward_sums = {} # Contains sum of rewards during episode\n",
    "all_state_visits = {} # Contains state visit counts during the last 10 episodes\n",
    "all_history = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_reward_sums['NN'] = []\n",
    "# all_reward_sums_sarsa_tile_4 = all_reward_sums['Sarsa']\n",
    "# all_reward_sums['Sarsa'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1\n",
    "num_episodes = 500\n",
    "Environment = envs['Grid-World']\n",
    "\n",
    "for algorithm in tqdm(list(agents.keys())):\n",
    "    all_reward_sums[algorithm] = []\n",
    "    all_state_visits[algorithm] = []\n",
    "    \n",
    "    for run in tqdm(range(num_runs)):\n",
    "        agent = agents[algorithm]()\n",
    "        env = Environment()\n",
    "        \n",
    "        env.env_init(env_info)\n",
    "        agent_info = {\"num_actions\": 4, \"num_states\": env.cols * env.rows, \"epsilon\": .1, \"step_size\": 0.5, \"discount\": 1} \n",
    "        agent_info[\"seed\"] = run\n",
    "        agent_info.update(agent_infos[algorithm])\n",
    "        np.random.seed(run)\n",
    "        agent.agent_init(agent_info)\n",
    "        \n",
    "        reward_sums = []\n",
    "        state_visits = np.zeros(env.cols * env.rows)\n",
    "        epsilon = 1\n",
    "        for episode in range(num_episodes):\n",
    "#             if episode < 50:\n",
    "#                 agent.epsilon = 1 \n",
    "#             else:\n",
    "#                 agent.epsilon = .1 \n",
    "#             print(f\"episode {episode}\",end='\\r')\n",
    "#             print(\"\")\n",
    "            agent.epsilon = epsilon\n",
    "            if episode < num_episodes - 10:\n",
    "                sum_of_rewards = run_episode(env, agent) \n",
    "            else: \n",
    "                # Runs an episode while keeping track of visited states and history\n",
    "                sum_of_rewards, history = run_episode(env, agent, state_visits, keep_history=True)\n",
    "                all_history.setdefault(algorithm, []).append(history)\n",
    "            epsilon *= 0.99\n",
    "            reward_sums.append(sum_of_rewards)\n",
    "        all_reward_sums[algorithm].append(reward_sums)\n",
    "        all_state_visits[algorithm].append(state_visits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('fpp': conda)",
   "language": "python",
   "name": "python37764bitfppcondae1fe1a88713143789471cb684e5828ba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
